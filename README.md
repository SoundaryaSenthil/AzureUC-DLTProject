Azure Data Engineering Project

with Azure DevOps, Unity Catalog, and Delta Live Tables (DLT)

This project showcases a complete end-to-end Azure Data Engineering solution using Azure Data Factory, Azure DevOps, and Azure Databricks integrated with Unity Catalog. 

It demonstrates how to build a robust, scalable, and governed data pipeline from raw data ingestion to curated insights using the Bronze-Silver-Gold Lakehouse Architecture.

![Screenshot 2025-04-18 190904](https://github.com/user-attachments/assets/5fcbec3f-a29a-4731-8a06-86e241fb60dd)
Components:

	â€¢	Source: GitHub Repository (CSV via HTTP API)
 
	â€¢	Ingestion Layer: Azure Data Factory (ADF)
 
	â€¢	Storage: Azure Data Lake Storage Gen2 (Raw, Bronze, Silver, Gold)
 
	â€¢	Transformation: Azure Databricks Notebooks + DLT Pipelines
 
	â€¢	Governance: Unity Catalog for centralized access control
 
	â€¢	Automation: Azure DevOps CI/CD for version control & deployment

â¸»

ğŸª£ ADLS Integration & Lakehouse Zoning

	â€¢	Created four containers: raw, bronze, silver, and gold
 
	â€¢	Implemented systematic folder structures for each data layer
 
	â€¢	Ensured secure and optimized access between Databricks and ADLS

â¸»

ğŸ”§ Azure DevOps Integration 	ğŸ”
	â€¢	Set up an Azure DevOps Repo with dedicated development branches
 
	â€¢	Linked ADF with DevOps for seamless source control & pipeline management
 
	â€¢	Enabled CI/CD pipelines to automate deployment

â¸»

ğŸ› ï¸ Azure Data Factory Pipelines	ğŸ”—

Built two dynamic, parameterized ADF pipelines:

	â€¢	Pipeline 1: GitHub â†’ Bronze
Ingests CSV files directly from GitHub using a parameterized HTTP URL

	â€¢	Pipeline 2: Raw â†’ Silver
Moves selected files based on matching conditions to structured layers in ADLS

â¸»

ğŸ§ª Azure Databricks + Unity Catalog	ğŸ§ 

	â€¢	Configured DBFS + ABFS integration to access ADLS
 
	â€¢	Registered External Locations in Unity Catalog for each zone
 
	â€¢	Created Silver Schema and performed transformations:
 
	â€¢	Handled null values, type casting, and duplicate checks
	â€¢	Applied window functions to compute cumulative weight by country
	â€¢	Cleaned and wrote data in Delta format

â¸»

âœ¨ Delta Live Tables (DLT) â€“ Gold Layer

	â€¢	Built a DLT Pipeline in Databricks for the Gold/Curated Layer
 
	â€¢	Streamed Delta-formatted Silver data
 
	â€¢	Created final Gold Delta Tables using declarative ETL logic in DLT

â¸»

ğŸŒŸ Key Features & Highlights

	â€¢	Dynamic Ingestion: Parameterized GitHub URL enables flexible, reusable pipelines
 
	â€¢	CI/CD with Azure DevOps: Track, test, and deploy all changes efficiently
 
	â€¢	Lakehouse Architecture: Bronze â†’ Silver â†’ Gold structured and governed zones
 
	â€¢	Real-time Transformation: Automated using DLT for streaming data
 
	â€¢	Delta Lake + DLT: ACID-compliant, scalable, and reliable data processing
 
	â€¢	Unity Catalog: Ensures centralized governance and secure access control.
