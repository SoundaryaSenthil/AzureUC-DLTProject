***AZURE DATA ENGINEERING PROJECT***

with Azure DevOps, Unity Catalog, and Delta Live Tables (DLT)

This project showcases a complete end-to-end Azure Data Engineering solution using Azure Data Factory, Azure DevOps, and Azure Databricks integrated with Unity Catalog. 

It demonstrates how to build a robust, scalable, and governed data pipeline from raw data ingestion to curated insights using the Bronze-Silver-Gold Lakehouse Architecture.

![Screenshot 2025-04-18 190904](https://github.com/user-attachments/assets/5fcbec3f-a29a-4731-8a06-86e241fb60dd)


***ARCHITECTURE:***
	
ðŸ§© Source: GitHub Repository (CSV files accessed via API)
	
 ðŸ§© Ingestion Layer: Azure Data Factory (ADF)
	
 ðŸ§© Storage: Azure Data Lake Storage Gen2 (Bronze, Silver, Gold zones)
	
 ðŸ§© Transformation: Azure Databricks (using notebooks and DLT pipelines)
	
 ðŸ§© Governance and Metadata: Unity Catalog in Databricks
	
 ðŸ§© Automation: Azure DevOps (CI/CD)

****KEY COMPONENTS***

***ðŸŽ¯ADLS Integration and Bronze-Silver-Gold Zones***

â€¢ Created four containers in ADLS for the Raw, Bronze, Silver, and Gold layers.

â€¢ Set up folder structures to organize raw, transformed, and curated data systematically.
	
***ðŸŽ¯ Azure DevOps Integration***

â€¢ Created an Azure DevOps account and set up a development branch.

â€¢ Connected Azure Data Factory (ADF) with Azure DevOps for version control and pipeline management.
	
***ðŸŽ¯ Azure Data Factory Pipelines***

â€¢ Built two parameterized ADF pipelines:

  1) GitHub to Bronze container in ADLS Gen 2:

     Ingests data directly from GitHub using a parameterized HTTP URL.
  
  2) Raw to Silver Layer Ingestion: 

     Ingests necessary files only if matches the condition from Azure Data Lake Storage (ADLS) to appropriate layers within the lake.

***ðŸŽ¯ Azure Databricks with Unity Catalog***
 
â€¢ Integrated Azure Databricks with ADLS using the DB connector.

â€¢ Set up Unity Catalog with external locations for Bronze, Silver, and Gold zones.

â€¢ Created a schema for the Silver layer and performed the following transformations:

  - Handled null values and data type conversions (e.g., string to float). 
  - Applied a window function to calculate the cumulative weight of athletes by country. 
  - Performed duplicate checks and more and cleaned the data

â€¢ Saved the transformed data in Delta format.

***ðŸŽ¯ Delta Live Tables (DLT) in Gold Layer*** 
 
â€¢ Created a DLT pipeline in the Gold (Curated) layer.

â€¢ Streamed Delta files from the Silver layer.

â€¢ Built final curated Delta tables using Databricksâ€™ ETL framework (DLT).

***KEY FEATURES & HIGHLIGHTS:***

â€¢ Dynamic Ingestion: Parameterized GitHub URL allows flexible ingestion of different files without modifying the pipeline logic.

â€¢ CI/CD with DevOps: All changes are tracked, versioned, and deployed using Azure DevOps pipelines.

â€¢ Layered Lakehouse Architecture: Clean separation between raw, transformed, and curated data using the Bronze-Silver-Gold model.

â€¢ Automated Streaming: Real-time data ingestion and transformation with minimal manual intervention.

â€¢ Delta Lake + DLT: Reliable, scalable data storage and processing with ACID transactions and schema enforcement.

â€¢ Governance with Unity Catalog: Centralized data governance and access control across all layers.

